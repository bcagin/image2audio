{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-81pRhC7nAU",
        "outputId": "1b1e1366-7fac-4291-ced2-43761a65bed3"
      },
      "outputs": [],
      "source": [
        "pip install --upgrade openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSXjEOuOrzIR",
        "outputId": "82b6c08c-550e-4620-dd30-d759416d6fe5"
      },
      "outputs": [],
      "source": [
        "pip install safetensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uc-4bLWA1MYn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    BlipProcessor,\n",
        "    BlipForConditionalGeneration,\n",
        "    MusicgenForConditionalGeneration,\n",
        "    AutoProcessor,\n",
        "    pipeline,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForTextToWaveform\n",
        ")\n",
        "from PIL import Image\n",
        "import torchaudio\n",
        "import openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxx-UzRE7rPK"
      },
      "outputs": [],
      "source": [
        "client = openai.OpenAI(api_key=\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQfnJELo8L6r"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login(token=\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yapt9Ssv5sE8",
        "outputId": "8db57925-b8dc-4412-b866-bd067ffeaf9b"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "OnUQkx7V1OwH"
      },
      "outputs": [],
      "source": [
        "def generate_image_caption(image_path):\n",
        "    # Set the path to your fine-tuned model\n",
        "    model_path = \"/content/drive/MyDrive/fine-tuned-blip-model/model\"\n",
        "\n",
        "    # Load the processor from Hugging Face\n",
        "    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "    # Load your fine-tuned BLIP model\n",
        "    model = BlipForConditionalGeneration.from_pretrained(model_path)\n",
        "\n",
        "    # Move model to GPU if available\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "\n",
        "    # Load and preprocess the image\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    inputs = processor(images=image, return_tensors=\"pt\").to(device)  # Move inputs to the same device\n",
        "\n",
        "    # Generate caption\n",
        "    with torch.no_grad():\n",
        "        caption_ids = model.generate(**inputs)\n",
        "\n",
        "    caption = processor.batch_decode(caption_ids, skip_special_tokens=True)[0]\n",
        "    return caption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LP5GGkRI78yq"
      },
      "outputs": [],
      "source": [
        "# Corrected GPT-4 function with latest API\n",
        "def refine_caption_gpt4(food_caption):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an AI food writer that enhances food descriptions with expressive, mouthwatering, and sensory-rich language for audio generation. Always add adjectives like 'tasty', 'savory', 'sweet', 'crispy', 'juicy', 'sizzling', 'buttery', 'rich', or 'delicious'.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Convert this food description into an expressive audio caption: {food_caption}\"}\n",
        "        ],\n",
        "        max_tokens=60\n",
        "    )\n",
        "\n",
        "    refined_caption = response.choices[0].message.content.strip()\n",
        "    return refined_caption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WfhFfbC-1TsN"
      },
      "outputs": [],
      "source": [
        "# Function to generate audio from caption\n",
        "def generate_audio(caption, output_path=\"output_audio.wav\"):\n",
        "    # Load MusicGen model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"csc-unipd/tasty-musicgen-small\")\n",
        "    model = AutoModelForTextToWaveform.from_pretrained(\"csc-unipd/tasty-musicgen-small\")\n",
        "\n",
        "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model.to(device)\n",
        "\n",
        "    print(f\"\\nðŸŽµ Generating music for caption: {caption}\")\n",
        "\n",
        "    # Process input text for MusicGen\n",
        "    inputs = tokenizer(caption, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Generate music\n",
        "    with torch.no_grad():\n",
        "        audio_output = model.generate(**inputs.to(device), do_sample=True, guidance_scale=3, max_new_tokens=256)\n",
        "\n",
        "    # Save audio file\n",
        "    torchaudio.save(output_path, audio_output[0].cpu(), sample_rate=32000)\n",
        "    print(f\"âœ… Music saved: {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "pJF0eS711VDr"
      },
      "outputs": [],
      "source": [
        "# Main function to integrate all steps\n",
        "def main(image_path, output_audio_path=\"output_audio.wav\"):\n",
        "    print(\"Generating food caption from image...\")\n",
        "    food_caption = generate_image_caption(image_path)\n",
        "    print(f\"Generated Food Caption: {food_caption}\")\n",
        "\n",
        "    print(\"Refining caption using GPT-4...\")\n",
        "    refined_caption = refine_caption_gpt4(food_caption)\n",
        "    print(f\"GPT-4 Refined Caption: {refined_caption}\")\n",
        "\n",
        "    print(\"Generating audio from refined caption...\")\n",
        "    generate_audio(refined_caption, output_audio_path)\n",
        "    print(\"Process complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydRipYCh1Wek",
        "outputId": "24a9e0b5-deb6-40b9-fef7-13e61e60c7b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating food caption from image...\n",
            "Generated Food Caption: a plate of sushi with shrimp, shrimp, and other sushi\n",
            "Refining caption using GPT-4...\n",
            "GPT-4 Refined Caption: \"Feast your ears on this, we have before us, a vibrant plate piled with succulent sushi. Highlighting plump, briny shrimp that command center stage, each piece is lovingly glazed with a glossy sheen. Delicate, silky ribbons of the freshest catch of the\n",
            "Generating audio from refined caption...\n",
            "\n",
            "ðŸŽµ Generating music for caption: \"Feast your ears on this, we have before us, a vibrant plate piled with succulent sushi. Highlighting plump, briny shrimp that command center stage, each piece is lovingly glazed with a glossy sheen. Delicate, silky ribbons of the freshest catch of the\n",
            "âœ… Music saved: output_audio.wav\n",
            "Process complete.\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    image_path = \"test_002572.jpg\"  # Replace with your image file path\n",
        "    main(image_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
